{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4584fac",
   "metadata": {},
   "source": [
    "# Rewriting DDSP components in PyTorch from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ddc944",
   "metadata": {},
   "source": [
    "## Some imports for plotting and audio plalyback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227be828",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [17, 6]\n",
    "from IPython.display import Audio\n",
    "from ipywidgets import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0a7b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # noqa\n",
    "from torch import Tensor\n",
    "\n",
    "from fftconv import fft_conv\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a0815c",
   "metadata": {},
   "source": [
    "## Utility functions for plotting signals and FFTs (NOT STFTs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f4e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wave(y, sr, title=None):\n",
    "    n_samples = len(y)\n",
    "    n_seconds = n_samples / sr\n",
    "    plt.plot(np.linspace(0, n_seconds, n_samples), y)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fft(fft, sr, title=None):\n",
    "    freqs = np.fft.rfftfreq(2 * len(fft) - 1, 1/sr)\n",
    "    plt.plot(freqs, fft)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcab476",
   "metadata": {},
   "source": [
    "## Utility function for showing audio controls with title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c8cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_audio(data, rate, title=None):\n",
    "    template = \"\"\"\n",
    "    <figure>\n",
    "        <figcaption>{title}:</figcaption>\n",
    "        {data}\n",
    "    </figure>\n",
    "    \"\"\"\n",
    "    return HTML(template.format(data=Audio(data, rate=rate)._repr_html_(), title=title))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f68acd",
   "metadata": {},
   "source": [
    "## Some constants, which will be DDSP/Network hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cef042",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 16000\n",
    "hop_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a455c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, sample_rate = librosa.load('/home/kureta/Music/violin/Violin Samples/yee_bach_dance_D#52.wav', sample_rate)\n",
    "signal = torch.from_numpy(signal[None, None, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e3607f",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- kernel dimensions in fft_conv are (out_channels, in_channels, size)\n",
    "- signal dimensions are (batch, channels, length)\n",
    "- I might need to implement my own FFT convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c75eb7c",
   "metadata": {},
   "source": [
    "# Reverb\n",
    "- Data coming to reverb has the shape (batch, features, sequence) (features = num_channels?).\n",
    "- If operating in realtime mode, we keep a buffer of reverb tails to add onto the next piece of audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wave(signal[0, 0].numpy(), sample_rate, title='Original audio')\n",
    "Audio(signal[0, 0], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b234cc5",
   "metadata": {},
   "source": [
    "Preparing an impulse response, which results in the original audio plus half a second delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0edc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = torch.zeros(1, 1, sample_rate)\n",
    "ir[0, 0, 0] = 1.0\n",
    "ir[0, 0, sample_rate // 2] = 1.0\n",
    "ir = ir.flip(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f539d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wave(ir[0, 0].flip(-1).numpy(), sample_rate, 'Impulse Response')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571ec657",
   "metadata": {},
   "source": [
    "Do the actual convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d9db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    result = fft_conv(F.pad(signal, (ir.shape[-1]-1, ir.shape[-1])), ir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8326441",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wave(result[0, 0].numpy(), sample_rate, title='With 0.5 sec. delay')\n",
    "Audio(result[0, 0], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304ffecd",
   "metadata": {},
   "source": [
    "# Final Reverb Class\n",
    "Works both offline and realtime\n",
    "\n",
    "## TODOs:\n",
    "- [ ] Reverberated signal is too loud. IR should be normalized or something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a52f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reverb(nn.Module):\n",
    "    def __init__(self, sample_rate=16000, duration=1.0, batch_size=1, live=False, n_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.duration = int(sample_rate * duration)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.live = live\n",
    "        self.n_channels = n_channels\n",
    "        \n",
    "        # ir.shape = (out_channels, in_channels, size)\n",
    "        self.ir = nn.Parameter(torch.rand(n_channels, n_channels, self.duration) * 2.0 - 1.0, requires_grad=True)\n",
    "        self.register_buffer('buffer', torch.zeros(self.batch_size, n_channels, self.duration), persistent=False)\n",
    "    \n",
    "    def forward(self, signal):\n",
    "        if self.live:\n",
    "            with torch.no_grad():\n",
    "                return self.forward_live(signal)\n",
    "        else:\n",
    "            return self.forward_learn(signal)\n",
    "    \n",
    "    def forward_learn(self, signal):\n",
    "        ir = self.ir.flip(-1)\n",
    "        signal_length = signal.shape[-1]\n",
    "        \n",
    "        result = fft_conv(F.pad(signal, (self.duration-1, self.duration)), ir)\n",
    "        \n",
    "        return result[..., :signal_length]\n",
    "    \n",
    "    def forward_live(self, signal):\n",
    "        ir = self.ir.flip(-1)\n",
    "        signal_length = signal.shape[-1]\n",
    "        \n",
    "        # Do the thing\n",
    "        result = fft_conv(F.pad(signal, (self.duration-1, self.duration)), ir)\n",
    "        \n",
    "        # Separate reverberated signal and tail\n",
    "        out = result[..., :signal_length]\n",
    "        tail = result[..., signal_length:]\n",
    "        \n",
    "        # add AT MOST first signal_length samples of the old buffer to the result\n",
    "        # reverb duration might be shorter than signal length. In that case, tail of the previous signal\n",
    "        # is shorter than the current signal.\n",
    "        previous_tail = self.buffer[..., :signal_length]\n",
    "        prev_tail_len = previous_tail.shape[-1]\n",
    "        out[..., :prev_tail_len] += previous_tail\n",
    "        \n",
    "        # zero out used samples of the old buffer\n",
    "        self.buffer[..., :prev_tail_len] = 0.0\n",
    "        \n",
    "        # roll used samples to the end\n",
    "        self.buffer = self.buffer.roll(-prev_tail_len, dims=-1)\n",
    "        \n",
    "        # add new tail to buffer\n",
    "        self.buffer += tail\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1943c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverb = Reverb(batch_size=10, n_channels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b8b33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, sample_rate = librosa.load('/home/kureta/Music/violin/Violin Samples/yee_bach_dance_D#52.wav', sample_rate)\n",
    "signal = torch.from_numpy(signal)\n",
    "while signal.ndim < 3:\n",
    "    signal.unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea06341",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = signal.repeat(10, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2c78e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverb.live = True\n",
    "result = []\n",
    "with torch.no_grad():\n",
    "    for i in range(53):\n",
    "        result.append(reverb(signal[..., i*855:(i+1)*855]))\n",
    "result = torch.cat(result, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e69c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverb.live = False\n",
    "with torch.no_grad():\n",
    "    result = reverb(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487824cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b38e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wave(result[0, 0].numpy(), sample_rate, title='With 0.5 second delay')\n",
    "Audio(result[0, :], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cdc9e3",
   "metadata": {},
   "source": [
    "# Noise\n",
    "For every control input generates `hop_size` length band filtered noise samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f49bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(hop_size=512):\n",
    "    return torch.rand(1, 1, hop_size) * 2.0 - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d224e",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_buffer = get_noise(hop_size * 2)\n",
    "windowed_buffer = torch.zeros(1, 1, 3 * hop_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7617a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = torch.zeros(100, 1, sample_rate // 2 + 1)\n",
    "for i in range(100):\n",
    "    bands[i, 0, i*2:i*2+110] = 1.0\n",
    "    bands[i, 0, i*40:i*40+110] = 1.0\n",
    "nir = torch.fft.irfft(bands, dim=-1)\n",
    "nir = torch.fft.fftshift(nir, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e049b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fft(bands[99, 0].numpy(), sample_rate, 'Noise filter bands (freqency domain)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0615f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wave(nir[99, 0].numpy(), sample_rate, 'Noise filter IR (time domain)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e3484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "with torch.no_grad():\n",
    "    for i in range(100):\n",
    "        f = fft_conv(F.pad(noise_buffer, (sample_rate-1, 0)), nir[i:i+1])\n",
    "        w = f * torch.hann_window(hop_size * 2)\n",
    "        windowed_buffer[..., -2*hop_size:] += w\n",
    "        r = windowed_buffer[..., -2*hop_size:-hop_size]\n",
    "        \n",
    "        windowed_buffer = windowed_buffer.roll(-hop_size, -1)\n",
    "        windowed_buffer[..., -hop_size:] = 0.0\n",
    "        noise_buffer = noise_buffer.roll(-hop_size, -1)\n",
    "        noise_buffer[..., -hop_size:] = get_noise(hop_size)\n",
    "        result.append(r)\n",
    "    \n",
    "    result = torch.cat(result, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4790302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wave(result[0, 0].numpy(), sample_rate, title='Filtered noise')\n",
    "Audio(result[0, 0], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95846f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft = np.fft.rfft(result[0, 0].numpy())\n",
    "plot_fft(np.abs(fft), sample_rate, 'FFT of filtered noise')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fd7549",
   "metadata": {},
   "source": [
    "# Noise Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc567e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(hop_size=512, batch_size=1):\n",
    "    return torch.rand(batch_size, 1, hop_size) * 2.0 - 1.0\n",
    "\n",
    "class Noise(nn.Module):\n",
    "    def __init__(self, sample_rate=16000, hop_size=512, batch_size=1):\n",
    "        super().__init__()\n",
    "        self.sample_rate = sample_rate\n",
    "        self.hop_size = hop_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.register_buffer('noise_buffer', get_noise(hop_size * 2, 1), persistent=False)\n",
    "        self.register_buffer('windowed_buffer', torch.zeros(batch_size, 1, 3 * hop_size), persistent=False)\n",
    "        self.register_buffer('window', torch.hann_window(hop_size * 2))\n",
    "    \n",
    "    def forward_live(self, bands):\n",
    "        bands = F.interpolate(bands, self.sample_rate // 2 + 1)\n",
    "        \n",
    "        # bands = F.interpolate(bands.unsqueeze(-1), (self.sample_rate // 2 + 1, 1), mode='bicubic', align_corners=True).squeeze(-1)\n",
    "        # bands = bands.clip(-1, 1)\n",
    "    \n",
    "        ir = torch.fft.irfft(bands, dim=-1)\n",
    "        ir = torch.fft.fftshift(ir, dim=-1)\n",
    "                \n",
    "        f = fft_conv(F.pad(self.noise_buffer, (self.sample_rate - 1, 0)), ir)\n",
    "        f = f.permute(1, 0, 2)\n",
    "        w = f * self.window\n",
    "        self.windowed_buffer[..., -2*self.hop_size:] += w\n",
    "        r = self.windowed_buffer[..., -2*self.hop_size:-self.hop_size]\n",
    "        self.windowed_buffer = self.windowed_buffer.roll(-self.hop_size, -1)\n",
    "        self.windowed_buffer[..., -self.hop_size:] = 0.0\n",
    "        self.noise_buffer = self.noise_buffer.roll(-self.hop_size, -1)\n",
    "        self.noise_buffer[..., -self.hop_size:] = get_noise(self.hop_size)\n",
    "        \n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46083648",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_gen = Noise(batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a760c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = torch.rand(10, 1, 100)\n",
    "bands = bands.repeat(1, 100, 1)\n",
    "for i in range(100):\n",
    "    bands[:, i, -i] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4bb574",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(100):\n",
    "    r = noise_gen.forward_live(bands[:, i:i+1])\n",
    "    result.append(r)\n",
    "result = torch.cat(result, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c570206",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wave(result[0, 0].numpy(), sample_rate, title='Filtered noise')\n",
    "Audio(result[0, 0], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13eb93b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nir = nir.reshape(100, 1, 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae43829",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.rand(1, 100, hop_size * 2)\n",
    "window = torch.hann_window(hop_size * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbb0444",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.rand(1, 1, 1, 100 * hop_size) * 2.0 - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823da0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfold = nn.Unfold(kernel_size=(1, hop_size * 2), stride=(1, hop_size), padding=(0, 512))\n",
    "fold = nn.Fold((1, hop_size*100), kernel_size=(1, hop_size*2), stride=(1, hop_size), padding=(0, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9607be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "framed_noise = unfold(noise)\n",
    "windowed_framed_noise = framed_noise * window.unsqueeze(0).unsqueeze(-1)\n",
    "windowed_framed_noise = windowed_framed_noise.unsqueeze(-2)\n",
    "convolved = fft_conv(F.pad(windowed_framed_noise, (15999, 0)), nir)\n",
    "unframed_noise = fold(convolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d5e3b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4584fac",
   "metadata": {},
   "source": [
    "# Rewriting DDSP components in PyTorch from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ddc944",
   "metadata": {},
   "source": [
    "## Some imports for plotting and audio plalyback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227be828",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [17, 6]\n",
    "from IPython.display import Audio\n",
    "from ipywidgets import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0a7b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # noqa\n",
    "from torch import Tensor\n",
    "\n",
    "from fftconv import fft_conv\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a0815c",
   "metadata": {},
   "source": [
    "## Utility functions for plotting signals and FFTs (NOT STFTs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f4e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wave(y, sr, title=None):\n",
    "    n_samples = len(y)\n",
    "    n_seconds = n_samples / sr\n",
    "    plt.plot(np.linspace(0, n_seconds, n_samples), y)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fft(fft, sr, title=None):\n",
    "    freqs = np.fft.rfftfreq(2 * len(fft) - 1, 1/sr)\n",
    "    plt.plot(freqs, fft)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcab476",
   "metadata": {},
   "source": [
    "## Utility function for showing audio controls with title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c8cb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def title_audio(data, rate, title=None):\n",
    "    template = \"\"\"\n",
    "    <figure>\n",
    "        <figcaption>{title}:</figcaption>\n",
    "        {data}\n",
    "    </figure>\n",
    "    \"\"\"\n",
    "    return HTML(template.format(data=Audio(data, rate=rate)._repr_html_(), title=title))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f68acd",
   "metadata": {},
   "source": [
    "## Some constants, which will be DDSP/Network hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cef042",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 16000\n",
    "hop_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a455c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, sample_rate = librosa.load('/home/kureta/Music/violin/Violin Samples/yee_bach_dance_D#52.wav', sample_rate)\n",
    "signal = torch.from_numpy(signal[None, None, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e3607f",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "- kernel dimensions in fft_conv are (out_channels, in_channels, size)\n",
    "- signal dimensions are (batch, channels, length)\n",
    "- I might need to implement my own FFT convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c75eb7c",
   "metadata": {},
   "source": [
    "# Reverb\n",
    "- Data coming to reverb has the shape (batch, features, sequence) (features = num_channels?).\n",
    "- If operating in realtime mode, we keep a buffer of reverb tails to add onto the next piece of audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wave(signal[0, 0].numpy(), sample_rate, title='Original audio')\n",
    "Audio(signal[0, 0], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b234cc5",
   "metadata": {},
   "source": [
    "Preparing an impulse response, which results in the original audio plus half a second delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0edc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = torch.zeros(1, 1, sample_rate)\n",
    "ir[0, 0, 0] = 1.0\n",
    "ir[0, 0, sample_rate // 2] = 1.0\n",
    "ir = ir.flip(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f539d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wave(ir[0, 0].flip(-1).numpy(), sample_rate, 'Impulse Response')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571ec657",
   "metadata": {},
   "source": [
    "Do the actual convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d9db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    result = fft_conv(F.pad(signal, (ir.shape[-1]-1, ir.shape[-1])), ir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8326441",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wave(result[0, 0].numpy(), sample_rate, title='With 0.5 sec. delay')\n",
    "Audio(result[0, 0], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304ffecd",
   "metadata": {},
   "source": [
    "# Final Reverb Class\n",
    "Works both offline and realtime\n",
    "\n",
    "## TODOs:\n",
    "- [ ] Reverberated signal is too loud. IR should be normalized or something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a52f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reverb(nn.Module):\n",
    "    def __init__(self, sample_rate=16000, duration=1.0, batch_size=1, live=False, n_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.duration = int(sample_rate * duration)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.live = live\n",
    "        self.n_channels = n_channels\n",
    "        \n",
    "        # ir.shape = (out_channels, in_channels, size)\n",
    "        self.ir = nn.Parameter(torch.rand(n_channels, n_channels, self.duration) * 2.0 - 1.0, requires_grad=True)\n",
    "        self.register_buffer('buffer', torch.zeros(self.batch_size, n_channels, self.duration), persistent=False)\n",
    "    \n",
    "    def forward(self, signal):\n",
    "        if self.live:\n",
    "            with torch.no_grad():\n",
    "                return self.forward_live(signal)\n",
    "        else:\n",
    "            return self.forward_learn(signal)\n",
    "    \n",
    "    def forward_learn(self, signal):\n",
    "        ir = self.ir.flip(-1)\n",
    "        signal_length = signal.shape[-1]\n",
    "        \n",
    "        result = fft_conv(F.pad(signal, (self.duration-1, self.duration)), ir)\n",
    "        \n",
    "        return result[..., :signal_length]\n",
    "    \n",
    "    def forward_live(self, signal):\n",
    "        ir = self.ir.flip(-1)\n",
    "        signal_length = signal.shape[-1]\n",
    "        \n",
    "        # Do the thing\n",
    "        result = fft_conv(F.pad(signal, (self.duration-1, self.duration)), ir)\n",
    "        \n",
    "        # Separate reverberated signal and tail\n",
    "        out = result[..., :signal_length]\n",
    "        tail = result[..., signal_length:]\n",
    "        \n",
    "        # add AT MOST first signal_length samples of the old buffer to the result\n",
    "        # reverb duration might be shorter than signal length. In that case, tail of the previous signal\n",
    "        # is shorter than the current signal.\n",
    "        previous_tail = self.buffer[..., :signal_length]\n",
    "        prev_tail_len = previous_tail.shape[-1]\n",
    "        out[..., :prev_tail_len] += previous_tail\n",
    "        \n",
    "        # zero out used samples of the old buffer\n",
    "        self.buffer[..., :prev_tail_len] = 0.0\n",
    "        \n",
    "        # roll used samples to the end\n",
    "        self.buffer = self.buffer.roll(-prev_tail_len, dims=-1)\n",
    "        \n",
    "        # add new tail to buffer\n",
    "        self.buffer += tail\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1943c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverb = Reverb(batch_size=10, n_channels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b8b33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, sample_rate = librosa.load('/home/kureta/Music/violin/Violin Samples/yee_bach_dance_D#52.wav', sample_rate)\n",
    "signal = torch.from_numpy(signal)\n",
    "while signal.ndim < 3:\n",
    "    signal.unsqueeze_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea06341",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = signal.repeat(10, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2c78e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverb.live = True\n",
    "result = []\n",
    "with torch.no_grad():\n",
    "    for i in range(53):\n",
    "        result.append(reverb(signal[..., i*855:(i+1)*855]))\n",
    "result = torch.cat(result, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e69c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverb.live = False\n",
    "with torch.no_grad():\n",
    "    result = reverb(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b38e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wave(result[0, 0].numpy(), sample_rate, title='With 0.5 second delay')\n",
    "Audio(result[0, :], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cdc9e3",
   "metadata": {},
   "source": [
    "# Noise\n",
    "For every control input generates `hop_size` length band filtered noise samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99726419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(batch_size=1, hop_size=512):\n",
    "    return torch.rand(batch_size, 1, hop_size) * 2.0 - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d182a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "seq_len = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417769ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_buffer = get_noise(batch_size, hop_size * 2)\n",
    "windowed_buffer = torch.zeros(batch_size, 1, 3 * hop_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e324afdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = torch.zeros(batch_size, seq_len, 1, sample_rate // 2 + 1)\n",
    "for i in range(seq_len):\n",
    "    bands[:, i, 0, i*1:i*1+110] = 10.0\n",
    "    bands[:, i, 0, i*40+1200:i*40+2200] = 1.0\n",
    "nir = torch.fft.irfft(bands, dim=-1)\n",
    "nir = torch.fft.fftshift(nir, dim=-1)\n",
    "nir = nir.permute(1, 0, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fft(bands[0, 0, 0].numpy(), sample_rate, 'Noise filter bands (freqency domain)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50727442",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "with torch.no_grad():\n",
    "    for i, ir in enumerate(nir):\n",
    "        f = fft_conv(F.pad(noise_buffer.view(1, -1, hop_size*2), (sample_rate-1, sample_rate)), ir.view(10, 1, 16000), groups=10)\n",
    "        f = f[..., sample_rate//2:-sample_rate//2]\n",
    "        w = f * torch.hann_window(hop_size * 2)\n",
    "        w = w.permute(1, 0, 2)\n",
    "        windowed_buffer[..., -2*hop_size:] += w\n",
    "        r = windowed_buffer[..., -2*hop_size:-hop_size]\n",
    "        \n",
    "        windowed_buffer = windowed_buffer.roll(-hop_size, -1)\n",
    "        windowed_buffer[..., -hop_size:] = 0.0\n",
    "        noise_buffer = noise_buffer.roll(-hop_size, -1)\n",
    "        noise_buffer[..., -hop_size:] = get_noise(batch_size, hop_size)\n",
    "        result.append(r)\n",
    "    \n",
    "    result = torch.cat(result, -1)\n",
    "    result[..., -512:] *= torch.hann_window(hop_size * 2)[-512:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76617807",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wave(result[1, 0].numpy(), sample_rate, title='Filtered noise')\n",
    "Audio(result[1, 0], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d073e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wave(result[0, 0].numpy(), sample_rate, title='Filtered noise')\n",
    "Audio(result[0, 0], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb2301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stft = librosa.stft(result[0, 0].numpy())\n",
    "librosa.display.specshow(librosa.amplitude_to_db(np.abs(stft)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5987442",
   "metadata": {},
   "source": [
    "## Get rid of the for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9251de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = torch.zeros(batch_size, seq_len, 1, sample_rate // 2 + 1)\n",
    "for i in range(seq_len):\n",
    "    bands[:, i, 0, i*1:i*1+110] = 10.0\n",
    "    bands[:, i, 0, i*40+1200:i*40+2200] = 1.0\n",
    "nir = torch.fft.irfft(bands, dim=-1)\n",
    "nir = torch.fft.fftshift(nir, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92a7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unfold = nn.Unfold(kernel_size=(1, hop_size * 2), stride=(1, hop_size), padding=(0, 0))\n",
    "# fold = nn.Fold((1, hop_size*seq_len + 2 * hop_size), kernel_size=(1, hop_size*2), stride=(1, hop_size), padding=(0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f38c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_buffer = torch.zeros(batch_size, 1, hop_size * 2) # get_noise(batch_size, hop_size * 2)\n",
    "noise = torch.rand(batch_size, 1, 1, seq_len * hop_size + hop_size) * 2.0 - 1.0\n",
    "framed_noise = unfold(noise)\n",
    "windowed_noise = framed_noise * torch.hann_window(hop_size*2).unsqueeze(0).unsqueeze(-1)\n",
    "windowed_noise = torch.cat([noise_buffer.permute(0, 2, 1), windowed_noise], dim=-1)\n",
    "print(windowed_noise.shape)\n",
    "re_noise = F.fold(windowed_noise, (1, hop_size*seq_len + 2 * hop_size), kernel_size=(1, hop_size*2), stride=(1, hop_size), padding=(0, 0))\n",
    "print(framed_noise.shape, noise.shape, re_noise.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc26ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "framed_noise = framed_noise.permute(0, 2, 1)\n",
    "framed_noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5905bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    f = fft_conv(F.pad(framed_noise.reshape(1, -1, hop_size*2), (sample_rate-1, sample_rate)), nir.reshape(batch_size*seq_len, 1, sample_rate), groups=batch_size*seq_len)\n",
    "    f = f[..., sample_rate//2:-sample_rate//2]\n",
    "    w = f * torch.hann_window(hop_size * 2)\n",
    "\n",
    "print(w.shape)\n",
    "w = w.reshape(10, 100, 1024)\n",
    "w = w.permute(0, 2, 1)\n",
    "print(w.shape)\n",
    "w, noise_buffer = torch.cat([noise_buffer.permute(0, 2, 1), w], dim=-1), w[:, : -1]\n",
    "print(w.shape)\n",
    "\n",
    "shit = F.fold(w, (1, hop_size*seq_len + 2 * hop_size), kernel_size=(1, hop_size*2), stride=(1, hop_size), padding=(0, 0))\n",
    "shit = shit.squeeze_(1)\n",
    "print(shit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a294c18d",
   "metadata": {},
   "source": [
    "first `2 hop_size` bit is the tail of last noise generated. So:\n",
    "\n",
    "- initialize a `2 * hop_size` `noise_buffer` to zero\n",
    "- generate noise `hop_size` longer than requested\n",
    "- unfold, filter, window\n",
    "- store the last element as the next `noise_buffer`\n",
    "- prepend the previous `noise_buffer` and overlap-add to `2 * hop_size` longer noise\n",
    "- drop first and last `hop_size` segments and return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea8fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wave(shit[1, 0, hop_size:-hop_size].numpy(), sample_rate, title='Filtered noise')\n",
    "Audio(shit[1, 0, hop_size:-hop_size], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc13be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stft = librosa.stft(shit[1, 0].numpy())\n",
    "librosa.display.specshow(librosa.amplitude_to_db(np.abs(stft)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf155cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

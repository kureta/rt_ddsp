{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227be828",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [17, 6]\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0a7b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # noqa\n",
    "from torch import Tensor\n",
    "\n",
    "from fftconv import fft_conv\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f4e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wave(y, sr):\n",
    "    n_samples = len(y)\n",
    "    n_seconds = n_samples / sr\n",
    "    plt.plot(np.linspace(0, n_seconds, n_samples), y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5512f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fft(fft, sr):\n",
    "    freqs = np.fft.rfftfreq(2 * len(fft) - 1, 1/sr)\n",
    "    plt.plot(freqs, fft)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a455c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, sample_rate = librosa.load('/home/kureta/Music/violin/Violin Samples/yee_bach_dance_D#52.wav', 16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c75eb7c",
   "metadata": {},
   "source": [
    "# Reverb\n",
    "Data coming to reveb has the shape (batch, features, sequence) (features = num_channels?).\n",
    "\n",
    "If operating in realtime mode, wee keep a buffer of reverb tails to add onto the next piece of audio.\n",
    "\n",
    "When reverb size is 16000 samples, tail turns out to be 16001 samples. I don't know what to do with that 1 residual sample but I guess I'll just discard the last one. It is 1 sample after all, shouldn't make much difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e712c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = torch.from_numpy(signal[None, None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(signal[0, 0], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d662001",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wave(signal[0, 0].numpy(), sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0edc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = torch.zeros(1, 1, 16000)\n",
    "ir[0, 0, 0] = 1.0\n",
    "ir[0, 0, 5000] = 1.0\n",
    "ir = ir.flip(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f539d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ir[0, 0].flip(-1).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f6e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "signal.shape, ir.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d9db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    result = fft_conv(signal, ir, padding=ir.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c844c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a335fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(result[0, 0], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103715b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(result[0, 0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304ffecd",
   "metadata": {},
   "source": [
    "# Final Reverb Class\n",
    "Works both offline and realtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a52f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reverb(nn.Module):\n",
    "    def __init__(self, sample_rate=16000, duration=1.0, batch_size=1, live=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.duration = int(sample_rate * duration)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.live = live\n",
    "        \n",
    "        self.ir = nn.Parameter(torch.rand(self.duration) * 2.0 - 1.0, requires_grad=True)\n",
    "        self.register_buffer('buffer', torch.zeros(self.batch_size, 1, self.duration), persistent=False)\n",
    "    \n",
    "    def forward(self, signal):\n",
    "        if self.live:\n",
    "            with torch.no_grad():\n",
    "                return self.forward_live(signal)\n",
    "        else:\n",
    "            return self.forward_learn(signal)\n",
    "    \n",
    "    def forward_learn(self, signal):\n",
    "        ir = self.ir[None, None, :].flip(-1)\n",
    "        signal_length = signal.shape[-1]\n",
    "        \n",
    "        result = fft_conv(signal, ir, padding=self.duration)\n",
    "        \n",
    "        return result[..., :signal_length]\n",
    "    \n",
    "    def forward_live(self, signal):\n",
    "        ir = self.ir[None, None, :].flip(-1)\n",
    "        signal_length = signal.shape[-1]\n",
    "        \n",
    "        # TODO: Understand why this is so.\n",
    "        # Drop the last residual sample\n",
    "        result = fft_conv(signal, ir, padding=ir.shape[-1])[..., :-1]\n",
    "        \n",
    "        # Separate reverberated signal and tail\n",
    "        out = result[..., :signal_length]\n",
    "        tail = result[..., signal_length:]\n",
    "        \n",
    "        # add AT MOST first signal_length samples of the old buffer to the result\n",
    "        # reverb duration might be shorter than signal length. In that case, tail of the previous signal\n",
    "        # is shorter than the current signal.\n",
    "        previous_tail = self.buffer[..., :signal_length]\n",
    "        prev_tail_len = previous_tail.shape[-1]\n",
    "        out[..., :prev_tail_len] += previous_tail\n",
    "        \n",
    "        # zero out used samples of the old buffer\n",
    "        self.buffer[..., :prev_tail_len] = 0.0\n",
    "        \n",
    "        # roll used samples to the end\n",
    "        self.buffer = self.buffer.roll(-prev_tail_len, dims=-1)\n",
    "        \n",
    "        # add new tail to buffer\n",
    "        self.buffer += tail\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1943c311",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverb = Reverb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2c78e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverb.live = True\n",
    "result = []\n",
    "for i in range(53):\n",
    "    result.append(reverb(signal[..., i*855:(i+1)*855]))\n",
    "result = torch.cat(result, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e69c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverb.live = False\n",
    "result = reverb(signal.reshape(5, 1, 45315//5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34510a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b38e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(result[2, 0].detach(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b76f47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(result[0, 0].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cdc9e3",
   "metadata": {},
   "source": [
    "# Noise\n",
    "For every control input generates `hop_size` length band filtered noise samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1152eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = torch.zeros(1, 1, 16000-1+512)\n",
    "noise = torch.rand(1, 1, 512) * 2.0 - 1.0\n",
    "buffer[..., -512:] = noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7617a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = torch.zeros(100, 1, 8001)\n",
    "for i in range(100):\n",
    "    bands[i, 0, i*20:i*20+110] = 1.0\n",
    "    bands[i, 0, i*40:i*40+110] = 1.0\n",
    "nir = torch.fft.irfft(bands, dim=-1)\n",
    "nir = torch.fft.fftshift(nir, dim=-1)\n",
    "nir.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e049b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bands[99, 0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0615f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wave(nir[0, 0].numpy(), sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba5bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise.shape, nir.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e3484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "with torch.no_grad():\n",
    "    for i in range(100):\n",
    "        r = fft_conv(buffer, nir[i:i+1])\n",
    "        buffer = buffer.roll(-512, -1)\n",
    "        buffer[..., -512:] = torch.rand(1, 1, 512) * 2.0 - 1.0\n",
    "        result.append(r)\n",
    "    \n",
    "    result = torch.cat(result, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8a9d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d365c06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(result[0, 0], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8100b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wave(result[0, 0, :2048].numpy(), sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95846f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft = np.fft.rfft(result[0, 0].numpy())\n",
    "plot_fft(np.abs(fft), sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0288227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bok = torch.rand(16000) * 2.0 - 1.0\n",
    "shit = torch.cat(4 * [bok])\n",
    "Audio(shit, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67c6fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
